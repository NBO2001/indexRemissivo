{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _isLetter(letter):\n",
    "    # 0 a 47\n",
    "    # 58 a 63\n",
    "    # 91 a 96\n",
    "    # 123 a 126\n",
    "    # 8200,8230\n",
    "    \n",
    "    codeLetter = ord(letter)\n",
    "\n",
    "    return not ( 0 <= codeLetter <= 57 \n",
    "    or 58 <= codeLetter <= 63 \n",
    "    or 91 <= codeLetter <= 96 \n",
    "    or 123 <= codeLetter <= 126 \n",
    "    or 8200 <= codeLetter <= 8230)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "57"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ord(\"9\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'b'"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chr(98)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cleaningWord(word):\n",
    "\n",
    "    i = 0\n",
    "    n = len(word)\n",
    "    \n",
    "    while i < n and not _isLetter(word[i]):\n",
    "        i += 1\n",
    "\n",
    "    j = n\n",
    "    while j > 0 and not _isLetter(word[j-1]):\n",
    "        j -= 1\n",
    "    \n",
    " \n",
    "    word = word[i:j]\n",
    "    \n",
    "    return word.lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tf(palavra, doc):\n",
    "    # palavra : total de palavras que aparceu no doc\n",
    "    # doc : numero total de palavras do doc\n",
    "    return palavra/doc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def n_containing(palavra, docs):\n",
    "    # retorna o numero de documento que tem a palvra\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def idf(qntWords, docs):\n",
    "    # Palavra: total de documentos que tem a palavra\n",
    "    # docs: total de documentos\n",
    "    h= docs/(qntWords if qntWords > 0 else 1)\n",
    "    return math.log2(h)\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tfidf(palavra, doc,qntWords, docs):\n",
    "    return tf(palavra, doc) * idf(qntWords, docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loadStopWords():\n",
    "    file_ = open(\"../data/stopwords_br.txt\")\n",
    "    wordsStop = []\n",
    "    linhas = file_.readlines()\n",
    "\n",
    "    for line in linhas:\n",
    "        wordsStop.append(cleaningWord(line))\n",
    "\n",
    "    return wordsStop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def validate(word,stopW):\n",
    "    return not cleaningWord(word) in stopW\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getTotWords(file,stopWords,doc, start, end):\n",
    "    wordsInDoc = {}\n",
    "    totWord = 0\n",
    "\n",
    "    for idxLine in range(start, end+1):\n",
    "        \n",
    "        wos = [ cleaningWord(x) for x in file[idxLine].split(\" \") if validate(x,stopWords)]\n",
    "\n",
    "        for wo in wos:\n",
    "            totWord += 1\n",
    "            if not wo in wordsInDoc.keys():\n",
    "                wordsInDoc[wo] = 1\n",
    "            else:\n",
    "                wordsInDoc[wo] += 1\n",
    "    return wordsInDoc, totWord"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mappingPages(file):\n",
    "    totLines = len(file)\n",
    "\n",
    "    pages = {}\n",
    "    startPage = 0\n",
    "    endPage = 0\n",
    "    ultPage = 1\n",
    "    i = 1\n",
    "\n",
    "    while i < totLines:\n",
    "\n",
    "        subStr = file[i][:2]\n",
    "\n",
    "        if (subStr == \"PA\"):\n",
    "            endPage = i-1\n",
    "            pages[ultPage] = {\n",
    "                \"start\":startPage,\n",
    "                \"end\": endPage\n",
    "            }\n",
    "\n",
    "            startPage = i\n",
    "            ultPage += 1\n",
    "\n",
    "        i+=1\n",
    "\n",
    "\n",
    "    endPage = i-1\n",
    "    pages[ultPage] = {\n",
    "        \"start\":startPage,\n",
    "        \"end\": endPage\n",
    "    }\n",
    "\n",
    "    return pages\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprareTable(dicResult, totDocs, totWords):\n",
    "    \n",
    "    words = []\n",
    "    recurrences = []\n",
    "    pontuation = []\n",
    "    pagesOc = []\n",
    "\n",
    "    for wd in dicResult:\n",
    "        words.append(wd)\n",
    "        recurrences.append(dicResult[wd][\"totRecorences\"])\n",
    "\n",
    "        pont = 0\n",
    "        stringTmp = \"\"\n",
    "        for page in dicResult[wd][\"pages\"]:\n",
    "            tmp = tfidf(dicResult[wd][\"pages\"][page][\"recorrences\"], dicResult[wd][\"pages\"][page][\"totWord\"], dicResult[wd][\"totPages\"], totDocs)\n",
    "            pont += tmp\n",
    "            stringTmp = f'{stringTmp} -> {page}: {dicResult[wd][\"pages\"][page][\"totWord\"]}'\n",
    "\n",
    "        toP = dicResult[wd][\"totPages\"] if dicResult[wd][\"totPages\"] > 0 else 1\n",
    "\n",
    "        pont = pont/toP\n",
    "\n",
    "        pontuation.append( pont )\n",
    "        pagesOc.append( stringTmp )\n",
    "\n",
    "    \n",
    "    d = {'palavra': words, 'rec': recurrences, \"pont\": pontuation}\n",
    "        \n",
    "    return pd.DataFrame(data=d)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pages: 264\n"
     ]
    }
   ],
   "source": [
    "# file_ = open(\"../data/Paralelismo.base\")\n",
    "# ffile_ = open(\"../data/Guarani.base\")\n",
    "file_ = open(\"../data/Aventuras.base\")\n",
    "lines = file_.readlines()\n",
    "\n",
    "pages = mappingPages(lines)\n",
    "print(f\"Pages: {len(pages)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "words = {}\n",
    "wordsResul = {}\n",
    "\n",
    "wordCont = 0\n",
    "\n",
    "stopWords = loadStopWords()\n",
    "\n",
    "for page in pages:\n",
    "\n",
    "    start = pages[page][\"start\"]\n",
    "    end = pages[page][\"end\"]\n",
    "\n",
    "    words, totWord = getTotWords(lines,stopWords, page, start, end)\n",
    "    for wd in words:\n",
    "        if len(wd) == 0:\n",
    "            continue\n",
    "\n",
    "        wordCont += 1\n",
    "            \n",
    "        if not wd in wordsResul.keys():\n",
    "            wordsResul[wd] = {\n",
    "                \"pages\":{\n",
    "                    page:{\n",
    "                        \"recorrences\": words[wd],\n",
    "                        \"totWord\": totWord\n",
    "                    }\n",
    "                },\n",
    "                \"totPages\": 1,\n",
    "                \"totRecorences\": words[wd]\n",
    "            }\n",
    "        else:\n",
    "            wordsResul[wd][\"pages\"][page] = {\n",
    "                \"recorrences\": words[wd],\n",
    "                \"totWord\": totWord\n",
    "            }\n",
    "            wordsResul[wd][\"totPages\"] += 1\n",
    "            wordsResul[wd][\"totRecorences\"] += words[wd]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "        palavra  rec      pont\n",
      "4816      sonho    5  0.132746\n",
      "8122   exéquias    5  0.132746\n",
      "6678      vinde    5  0.129748\n",
      "10705    cótica    5  0.127689\n",
      "11255  compadre   11  0.125992\n",
      "...         ...  ...       ...\n",
      "485       disse  151  0.005096\n",
      "431        pois  173  0.005076\n",
      "1058      mesmo  167  0.005022\n",
      "567          só  235  0.004285\n",
      "0            pa  264  0.000000\n",
      "\n",
      "[11581 rows x 3 columns]\n"
     ]
    }
   ],
   "source": [
    "table = preprareTable(wordsResul, len(pages), wordCont)\n",
    "\n",
    "# table = table.loc[table[\"rec\"] > 1]\n",
    "# table = table.loc[table[\"rec\"] <= 5]\n",
    "table = table.sort_values(by=\"pont\", ascending=False)\n",
    "print(table)\n",
    "\n",
    "table.to_csv(\"Paralelismo.csv\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py36",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4 (main, Mar 31 2022, 08:41:55) [GCC 7.5.0]"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "04dcb3f6a367c728cd23f66085c9c7d5d2b5cfc39fd5eebfcaa944146d36fa18"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
